{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95601f80",
   "metadata": {},
   "source": [
    "## Created By: Prateek Kakkar and Ramgopal Reddy Putta\n",
    "### Contact Information: pkakkar@seattleu.edu and rputta@seattleu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bc5dc",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bce871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq pypdfium2\n",
    "!pip install -Uqq tabula-py\n",
    "!pip install -Uqq pdfplumber\n",
    "!pip install -Uqq python-dateutil\n",
    "!pip install -Uqq xlrd\n",
    "\n",
    "!pip install -Uqq regex\n",
    "!pip install -Uqq camelot-py[cv]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc613138",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901d836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pypdfium2 as pdfium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import uuid\n",
    "import camelot as cam\n",
    "from numpy.core.fromnumeric import transpose\n",
    "from pathlib import Path\n",
    "import tkinter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c3784",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d58da5",
   "metadata": {},
   "source": [
    "#### Tera Nova Extract Text Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tera Nova Extract Text Fields# This function takes pdf as an input\n",
    "# converts into text\n",
    "# returns the required fields\n",
    "    \n",
    "def TeraNovaText(folder,file,prjct_id,scrpr_id):\n",
    "    pdf_fium = pdfium.PdfDocument(folder+'/'+file) # Read the PDF as a text File\n",
    "    pdf_plumb = pdfplumber.open(folder+'/'+file) # Read the PDF as a text File\n",
    "    \n",
    "    no_of_pages = len(pdf_fium)\n",
    "    \n",
    "    out= []  # to store the output of this function\n",
    "    \n",
    "    for i in range(no_of_pages):\n",
    "        temp_out = []\n",
    "        \n",
    "        page_fium = pdf_fium[i] # pdfium\n",
    "        page_plumb = pdf_plumb.pages[i] # PDF Plumber\n",
    "        \n",
    "        # Load a text page helper\n",
    "        textpage_fium = page_fium.get_textpage() # pdfium\n",
    "    \n",
    "        # Extract text from the whole page\n",
    "        text_fium = textpage_fium.get_text_range() # pdfium\n",
    "        text_plumb = page_plumb.extract_text() # PDF Plumber\n",
    "    \n",
    "        # Split the list with the delimiters\n",
    "        list_fium = text_fium.split('\\r\\n') # pdfium\n",
    "        list_plumb = text_plumb.split('\\n')  # PDF Plumber\n",
    "    \n",
    "    \n",
    "        # Store the requird fields\n",
    "    \n",
    "        # Data Ingestion_datetime\n",
    "        Ingestion_datetime = pd.to_datetime('today').strftime(\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    \n",
    "        # City\n",
    "        #City = list_plumb[3].split(',')[0].split(' ')[-1]\n",
    "    \n",
    "        # DATE MOLDED\n",
    "        date_molded_src = \"DATE MOLDED\"\n",
    "        date_molded_src_get_string = [x for x in list_fium if date_molded_src in x]\n",
    "        date_molded_src_lst_str = ''.join(date_molded_src_get_string)\n",
    "        Date_Molded = date_molded_src_lst_str.split(' ')[2]\n",
    "    \n",
    "        # DATE ISSUED\n",
    "        date_issue_src = \"DATE ISSUED\"\n",
    "        date_issue_src_get_string = [x for x in list_fium if date_issue_src in x]\n",
    "        date_issue_src_lst_str = ''.join(date_issue_src_get_string)\n",
    "        Date_Issued = date_issue_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # LAB NUMBER\n",
    "        Lab_Number = date_molded_src_lst_str.split(' ')[5]\n",
    "    \n",
    "        # LOCATION OF PLACEMENT\n",
    "        colon = \":\"\n",
    "        lop_searc = \"LOCATION OF PLACEMENT\"\n",
    "        lop_searc_get_string = [x for x in list_fium if lop_searc in x]\n",
    "        lop_searc_lst_str = ''.join(lop_searc_get_string)\n",
    "\n",
    "        loc = lop_searc_lst_str\n",
    "\n",
    "        if any(c in colon for c in loc):\n",
    "            Location_of_Placement = loc.split(':')[1].strip()\n",
    "        else:\n",
    "            loc = loc.split(' ')\n",
    "            loc.insert(3, ':')\n",
    "            loc = \" \".join(loc) \n",
    "            Location_of_Placement = loc.split(':')[1].strip()\n",
    "        \n",
    "        # CONCRETE SUPPLIER\n",
    "        supplier_src = \"CONCRETE SUPPLIER\"\n",
    "        supplier_src_get_string = [x for x in list_plumb if supplier_src in x]\n",
    "        supplier_src_lst_str = ''.join(supplier_src_get_string)\n",
    "        Concrete_Supplier = supplier_src_lst_str.split('WATER')[0].split(':')[1].strip()\n",
    "        \n",
    "        # MIX ID NO\n",
    "        mixid_src = \"MIX ID NO\"\n",
    "        mixid_src_get_string = [x for x in list_plumb if mixid_src in x]\n",
    "        mixid_src_lst_str = ''.join(mixid_src_get_string)\n",
    "        Mix_ID_No = mixid_src_lst_str.split('UNIT')[0].split(':')[1].strip()\n",
    "    \n",
    "        # WEATHER\n",
    "        weather_src = \"WEATHER\"\n",
    "        weather_src_get_string = [x for x in list_fium if weather_src in x]\n",
    "        if weather_src_get_string:\n",
    "            weather_src_lst_str = ''.join(weather_src_get_string)\n",
    "            Weather = weather_src_lst_str.split(':')[1].strip()\n",
    "            \n",
    "        else:\n",
    "            weather_src = \"W EATHER\"\n",
    "            weather_src_get_string = [x for x in list_fium if weather_src in x]\n",
    "            weather_src_lst_str = ''.join(weather_src_get_string)\n",
    "            Weather = weather_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # TIME MOLDED\n",
    "        time_src = \"TIME MOLDED\"\n",
    "        time_src_get_string = [x for x in list_plumb if time_src in x]\n",
    "        time_src_lst_str = ''.join(time_src_get_string)\n",
    "        Time_Molded = time_src_lst_str.split(' ')[2] + ' ' +time_src_lst_str.split(' ')[3]\n",
    "        if \"ASTM\" in Time_Molded:\n",
    "            Time_Molded = \"\"\n",
    "        else:\n",
    "            Time_Molded = Time_Molded[0:8]\n",
    "    \n",
    "        # AIR CONTENT(%)\n",
    "        aircontent_src = \"AIR CONTENT\"\n",
    "        aircontent_src_get_string = [x for x in list_fium if aircontent_src in x]\n",
    "        aircontent_src_lst_str = ''.join(aircontent_src_get_string)\n",
    "        Air_Content = aircontent_src_lst_str.split(':')[1].strip()\n",
    "        Air_Content = re.sub('[^0-9,.]', '', Air_Content)\n",
    "    \n",
    "        # SLUMP(IN)\n",
    "        slump_src = \"SLUMP(IN)\"\n",
    "        slump_src_get_string = [x for x in list_fium if slump_src in x]\n",
    "        slump_src_lst_str = ''.join(slump_src_get_string)\n",
    "        Slump_space_chck = slump_src_lst_str.split(':')\n",
    "        if len(Slump_space_chck[1])>5:\n",
    "            Slump = Slump_space_chck[1].split(' ')[1].strip()\n",
    "        else:\n",
    "            Slump = Slump_space_chck[1].strip()\n",
    "        \n",
    "        Slump = re.sub('[^0-9,.]', '', Slump)\n",
    "        \n",
    "    \n",
    "        # SIZE & REQUIRED PSI \n",
    "        size_psi_search = \"SIZE:\"\n",
    "        size_psi_get_string = [x for x in list_plumb if size_psi_search in x]\n",
    "        if size_psi_get_string:\n",
    "            size_psi_lst_str = ''.join(size_psi_get_string)\n",
    "            Size = size_psi_lst_str.split('DIAMETER')[0].split(':')[-1].strip()\n",
    "            Required_PSI = size_psi_lst_str.split(' ')[-1].strip()\n",
    "        else:\n",
    "            size_psi_search = \"SIZE\"\n",
    "            size_psi_get_string = [x for x in list_plumb if size_psi_search in x]\n",
    "            size_psi_lst_str = ''.join(size_psi_get_string)\n",
    "            Size = size_psi_lst_str.split('DIAMETER')[0].split('SIZE')[-1].strip()\n",
    "            Required_PSI = size_psi_lst_str.split(' ')[-1].strip()\n",
    "        \n",
    "        # WATER ADDED(GALS)    \n",
    "        water_searc = \"GALS\"\n",
    "        water_searc_get_string = [x for x in list_fium if water_searc in x]\n",
    "        water_searc_lst_str = ''.join(water_searc_get_string)\n",
    "        Water_Added_len_check = water_searc_lst_str.split(':')\n",
    "        \n",
    "        if len(Water_Added_len_check[1])>5:\n",
    "            Water_Added = Water_Added_len_check[1].split(' ')[1].strip()\n",
    "        else:\n",
    "            Water_Added = Water_Added_len_check[1].strip()\n",
    "        \n",
    "        Water_Added = re.sub('[^0-9,.]', '', Water_Added)\n",
    "    \n",
    "    \n",
    "        # UNIT WEIGHT(PCF):\n",
    "        unit_searc = \"PCF\"\n",
    "        unit_searc_get_string = [x for x in list_fium if unit_searc in x]\n",
    "        unit_searc_lst_str = ''.join(unit_searc_get_string)\n",
    "        Unit_Weight = unit_searc_lst_str.split(':')[1].strip()\n",
    "        Unit_Weight = re.sub('[^0-9,.]', '', Unit_Weight)\n",
    "    \n",
    "        # AMBIENT TEMP(F):\n",
    "        ambi_temp_searc = \"AMBIENT TEMP\"\n",
    "        ambi_temp_searc_get_string = [x for x in list_fium if ambi_temp_searc in x]\n",
    "        ambi_temp_searc_lst_str = ''.join(ambi_temp_searc_get_string)\n",
    "        Ambient_Temp = ambi_temp_searc_lst_str.split(':')[1].strip()\n",
    "        Ambient_Temp = re.sub('[^0-9]', '', Ambient_Temp)\n",
    "    \n",
    "        # CONCRETE TEMP(F):\n",
    "        concrete_temp_searc = \"CONCRETE TEMP\"\n",
    "        concrete_temp_searc_get_string = [x for x in list_fium if concrete_temp_searc in x]\n",
    "        concrete_temp_searc_lst_str = ''.join(concrete_temp_searc_get_string)\n",
    "        Concrete_Temp = concrete_temp_searc_lst_str.split(':')[1].strip()\n",
    "        Concrete_Temp = re.sub('[^0-9]', '', Concrete_Temp)\n",
    "        \n",
    "        # PROJECT ID\n",
    "        ProjectID = prjct_id\n",
    "        \n",
    "        # SCRAPER ID\n",
    "        ScraperID = scrpr_id\n",
    "        \n",
    "        temp_out = [ProjectID,ScraperID,file,Ingestion_datetime,Date_Molded,Date_Issued,Lab_Number,Location_of_Placement\n",
    "       ,Concrete_Supplier,Mix_ID_No,Weather,Time_Molded,Air_Content,Slump,Size\n",
    "       ,Required_PSI,Water_Added,Unit_Weight,Ambient_Temp,Concrete_Temp]\n",
    "        \n",
    "        out.append((temp_out))\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29746d4f",
   "metadata": {},
   "source": [
    "#### Tera Nova Extract Test Report Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea56f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes pdf as an input\n",
    "# converts into a dataframe\n",
    "# returns the test results as a DataFrame\n",
    "\n",
    "def TeraNovaPdf2Table(folder,file,result):\n",
    "    readpdf2df = tabula.io.read_pdf(folder+'/'+file \n",
    "                             , pages='all'\n",
    "                             #,output_format=\"dataframe\"\n",
    "                                   , multiple_tables=True)\n",
    "    \n",
    "    df = pd.concat(readpdf2df)\n",
    "    age_load_read = df.iloc[1:,3].dropna().reset_index().drop(columns=['index']).iloc[:-1,]\n",
    "    \n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('diameter', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('compression', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('age', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    \n",
    "    # Extra Space in the Data Check\n",
    "    space_check = age_load_read.iloc[:,0].str.split(' ').tolist()\n",
    "    \n",
    "    space_check_ref = []\n",
    "    for i in space_check:\n",
    "        for j in i:\n",
    "            if len(i)>4:\n",
    "                i[1]=i[1]+i[2]\n",
    "                i.pop(2)\n",
    "        space_check_ref.append(i)\n",
    "    space_check = space_check_ref\n",
    "    age_load_read = pd.DataFrame(space_check, columns =['Age_Days','Date_Tested','Total_load', 'Unit_load'])\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in range(len(result)):\n",
    "        df_split = np.array_split(age_load_read, len(result))\n",
    "        \n",
    "        result_lab = result[i][6]\n",
    "        result_date_isssue = result[i][5]\n",
    "        \n",
    "        df_sel = df_split[i]\n",
    "        \n",
    "        iterat = df_sel.shape[0]\n",
    "        \n",
    "        lab_nbr = pd.DataFrame({'Lab_Number': result_lab}, index=[0])\n",
    "        date_isu = pd.DataFrame({'Date_Issued': result_date_isssue}, index=[0])\n",
    "        \n",
    "        lab_nbr_itr = pd.DataFrame(np.repeat(lab_nbr.values, iterat, axis=0))\n",
    "        date_isu_itr = pd.DataFrame(np.repeat(date_isu.values, iterat, axis=0))\n",
    "        \n",
    "        lab_nbr_itr.columns = lab_nbr.columns\n",
    "        date_isu_itr.columns = date_isu.columns\n",
    "        \n",
    "        temp_df = pd.concat([lab_nbr_itr,date_isu_itr,age_load_read.iloc[:, age_load_read.columns != 'Total_load']], axis=1, join='inner')\n",
    "        final_df = final_df.append(temp_df).reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb08202",
   "metadata": {},
   "source": [
    "#### TeraNova Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fec7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TeraNova(folder_dir,prjct_id,scrpr_id):\n",
    "    log = []\n",
    "    df_text = []\n",
    "    pdf_df = pd.DataFrame()\n",
    "\n",
    "    #path = '/Users/prateek/Documents/Captsone Project/Jupiter Files/TestFolder'\n",
    "    \n",
    "    #os.chdir('C:/Users/pkakkar/Documents/2023-02-17-DataSet-006-008')\n",
    "    #path = 'C:/Users/pkakkar/Documents/2023-02-17-DataSet-006-008'\n",
    "    \n",
    "    os.chdir(folder_dir)\n",
    "    path = folder_dir\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(path,topdown=True):\n",
    "        if not root.endswith('.DS_Store'):\n",
    "            for file in files:\n",
    "                if file.endswith('.pdf'):\n",
    "                    read_datetime = pd.to_datetime('today').strftime(\"%m/%d/%Y %I:%M:%S %p\")\n",
    "                    folder = os.path.basename(root)\n",
    "                    dir_path = os.path.dirname(root)\n",
    "                \n",
    "                    log.append((prjct_id,scrpr_id,dir_path,folder,file,read_datetime))  # Appends results for log DF\n",
    "                \n",
    "                    result = TeraNovaText(root,file,prjct_id,scrpr_id) # Calls text function\n",
    "                \n",
    "                    for j in range(len(result)):\n",
    "                        df_text.append((result[j]))    # Appends results from text function\n",
    "                     \n",
    "                    df_out = TeraNovaPdf2Table(root,file,result) # Calls Dataframe function\n",
    "                    pdf_df = pdf_df.append(df_out).reset_index(drop=True) # Appends test results DF\n",
    "                \n",
    "\n",
    "    # Append Log DataFrame        \n",
    "    df_log = pd.DataFrame(log, columns=['ProjectID', 'ScraperID','Directory','Folder','File', 'Read_DateTime'])\n",
    "\n",
    "    # Append DataFrame with Text fields\n",
    "    df_page = pd.DataFrame(df_text, columns=['ProjectID',\n",
    "                     'ReportScraperID',\n",
    "                     'File_Name',\n",
    "                     'Ingestion_datetime',\n",
    "                     'Date_Molded',\n",
    "                     'Date_Issued',\n",
    "                     'Lab_Number',\n",
    "                     'Location_of_Placement',\n",
    "                     'Concrete_Supplier',\n",
    "                     'Mix_ID_No',\n",
    "                     'Weather',\n",
    "                     'Time_Molded',\n",
    "                     'Air_Content(%)',\n",
    "                     'Slump(in)',\n",
    "                     'Size',\n",
    "                     'Required_PSI',\n",
    "                     'Water_Added(GALS)',\n",
    "                     'Unit_Weight(PCF)',\n",
    "                     'Ambient_Temp(F)',\n",
    "                     'Concrete_Temp(F)'])\n",
    "    \n",
    "    return df_page,pdf_df,df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec921f3",
   "metadata": {},
   "source": [
    "#### Tera Nova Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28dd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TeraNovaDataTransform(TeraNovaFieldReport, TeraNovaTestResult):\n",
    "    ############## Create a copy of the Dataframes  #############\n",
    "\n",
    "    TeraNovaFieldReport_stg = TeraNovaFieldReport.copy()\n",
    "    TeraNovaTestResult_stg = TeraNovaTestResult.copy()\n",
    "    \n",
    "    # generate the uuid for BatchLabID \n",
    "    batchlabID = {Lab_Number: str(uuid.uuid4()) for Lab_Number in TeraNovaFieldReport_stg['Lab_Number'].unique()}\n",
    "    \n",
    "    # map uuid\n",
    "    TeraNovaFieldReport_stg['BatchLabID'] = TeraNovaFieldReport_stg['Lab_Number'].map(batchlabID)\n",
    "    \n",
    "    # Add BatchLabID to Results Table\n",
    "    TeraNovaTestResult_stg['BatchLabID'] = TeraNovaTestResult_stg['Lab_Number'].map(batchlabID)\n",
    "    \n",
    "    \n",
    "    ###################### Rename the Columns ########################\n",
    "\n",
    "    TeraNovaFieldReport_stg = TeraNovaFieldReport_stg.rename(columns={'File_Name': 'ReportFileName'\n",
    "                                              , 'Date_Issued': 'ReportDateIssued'\n",
    "                                              , 'Ingestion_datetime': 'ReportTimeIngested'\n",
    "                                              , 'Ambient_Temp(F)': 'SiteTemperature'\n",
    "                                              , 'Weather': 'SiteWeather'\n",
    "                                              , 'Concrete_Supplier': 'ConcreteSupplier'\n",
    "                                              , 'Location_of_Placement': 'ConcretePlacementLocation'\n",
    "                                              , 'Lab_Number': 'BatchLabNumber'\n",
    "                                              , 'Mix_ID_No': 'BatchMixID'\n",
    "                                              , 'Size': 'BatchSpecimenSize'\n",
    "                                              , 'Concrete_Temp(F)': 'BatchTemperature'\n",
    "                                              , 'Unit_Weight(PCF)': 'BatchUnitWeight'\n",
    "                                              , 'Required_PSI': 'BatchRequiredStrength'\n",
    "                                              , 'Air_Content(%)': 'BatchAirContent'\n",
    "                                              , 'Slump(in)': 'BatchtSlump'\n",
    "                                              , 'Water_Added(GALS)': 'BatchWaterAdded'})\n",
    "\n",
    "\n",
    "\n",
    "    TeraNovaTestResult_stg = TeraNovaTestResult_stg.rename(columns={'Lab_Number': 'BatchLabNumber'\n",
    "                                              , 'Date_Issued': 'ReportDateIssued'\n",
    "                                              , 'Date_Tested': 'SpecimenDateTested'\n",
    "                                              , 'Unit_load': 'SpecimenMeasuredStrength'\n",
    "                                              , 'Age_Days': 'SpecimenAgeTested'})\n",
    "\n",
    "\n",
    "\n",
    "    ########### Changing Data Types of Columns for SpecimenFieldReport #########\n",
    "\n",
    "    TeraNovaFieldReport_stg['ReportTimeIngested'] = TeraNovaFieldReport_stg['ReportTimeIngested'].astype('datetime64[ns]')\n",
    "    TeraNovaFieldReport_stg['ReportDateIssued'] = pd.to_datetime(TeraNovaFieldReport_stg[\"ReportDateIssued\"], format=\"%d-%b-%y\")\n",
    "\n",
    "\n",
    "    TeraNovaFieldReport_stg['SiteTemperature'] = TeraNovaFieldReport_stg['SiteTemperature'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    TeraNovaFieldReport_stg['BatchTemperature'] = TeraNovaFieldReport_stg['BatchTemperature'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    TeraNovaFieldReport_stg['BatchUnitWeight'] = TeraNovaFieldReport_stg['BatchUnitWeight'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    TeraNovaFieldReport_stg['BatchRequiredStrength'] = TeraNovaFieldReport_stg['BatchRequiredStrength'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    TeraNovaFieldReport_stg['BatchAirContent'] = TeraNovaFieldReport_stg['BatchAirContent'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    TeraNovaFieldReport_stg['BatchtSlump'] = TeraNovaFieldReport_stg['BatchtSlump'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    TeraNovaFieldReport_stg['BatchWaterAdded'] = TeraNovaFieldReport_stg['BatchWaterAdded'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    #df_page_stg['BatchLabNumber'] = df_page_stg['BatchLabNumber'].astype('int')\n",
    "    TeraNovaFieldReport_stg['SiteTemperature'] = TeraNovaFieldReport_stg['SiteTemperature'].astype('float')\n",
    "    TeraNovaFieldReport_stg['BatchTemperature'] = TeraNovaFieldReport_stg['BatchTemperature'].astype('float')\n",
    "    TeraNovaFieldReport_stg['BatchUnitWeight'] = TeraNovaFieldReport_stg['BatchUnitWeight'].astype('float')\n",
    "    TeraNovaFieldReport_stg['BatchRequiredStrength'] = TeraNovaFieldReport_stg['BatchRequiredStrength'].astype('float')\n",
    "    TeraNovaFieldReport_stg['BatchAirContent'] = TeraNovaFieldReport_stg['BatchAirContent'].astype('float')\n",
    "    TeraNovaFieldReport_stg['BatchtSlump'] = TeraNovaFieldReport_stg['BatchtSlump'].astype('float')\n",
    "    TeraNovaFieldReport_stg['BatchWaterAdded'] = TeraNovaFieldReport_stg['BatchWaterAdded'].astype('float')\n",
    "    \n",
    "    TeraNovaFieldReport_stg['BatchSpecimenSize'] = TeraNovaFieldReport_stg['BatchSpecimenSize'].str.replace(' ','')\n",
    "\n",
    "\n",
    "    TeraNovaFieldReport_stg[\"Time_Molded\"] = TeraNovaFieldReport_stg['Time_Molded'].str.lower()\n",
    "    TeraNovaFieldReport_stg['Time_Molded'] = TeraNovaFieldReport_stg['Time_Molded'].replace({'sm':'am', 'om':'pm'}, regex = True)\n",
    "\n",
    "\n",
    "    TeraNovaFieldReport_stg[\"BatchTimeMolded\"] = TeraNovaFieldReport_stg[\"Date_Molded\"] + ' ' +TeraNovaFieldReport_stg[\"Time_Molded\"]\n",
    "    TeraNovaFieldReport_stg['BatchTimeMolded'] = TeraNovaFieldReport_stg['BatchTimeMolded'].astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "\n",
    "    ########### Changing Data Types of Columns for SpecimenCompressionTestResult #########\n",
    "\n",
    "    # Remove rows with not integer age days\n",
    "    TeraNovaTestResult_stg = TeraNovaTestResult_stg[pd.to_numeric(TeraNovaTestResult_stg['SpecimenAgeTested'], errors='coerce').notna()]\n",
    "\n",
    "    # Change ReportedDateIssues to datetime\n",
    "    TeraNovaTestResult_stg['ReportDateIssued'] = pd.to_datetime(TeraNovaTestResult_stg[\"ReportDateIssued\"], format=\"%d-%b-%y\")\n",
    "\n",
    "    # Create a new dataframe to handle junk datevalue\n",
    "    junk_df = pd.DataFrame()\n",
    "\n",
    "    # This code chnages SpecimenDateTested to datatime format\n",
    "    for i in TeraNovaTestResult_stg['SpecimenDateTested']:\n",
    "        if len(i)!= 9:\n",
    "            junk_df = junk_df.append(TeraNovaTestResult_stg.loc[TeraNovaTestResult_stg['SpecimenDateTested'] == i]).reset_index(drop=True)\n",
    "            TeraNovaTestResult_stg = TeraNovaTestResult_stg.drop(labels=[TeraNovaTestResult_stg.loc[TeraNovaTestResult_stg['SpecimenDateTested'] == i].index.values[0]]\n",
    "                                         , axis=0).reset_index(drop=True)\n",
    "\n",
    "    TeraNovaTestResult_stg['SpecimenDateTested'] = pd.to_datetime(TeraNovaTestResult_stg[\"SpecimenDateTested\"], format=\"%d-%b-%y\")\n",
    "\n",
    "    if junk_df.empty:\n",
    "        pass\n",
    "    else:\n",
    "        junk_df['SpecimenDateTested'] = pd.to_datetime(junk_df[\"SpecimenDateTested\"], format=\"%d-%m-%y\")\n",
    "        TeraNovaTestResult_stg = pd.concat([TeraNovaTestResult_stg, junk_df], ignore_index=True, sort=False)\n",
    "\n",
    "    del junk_df   # Delete Junk dataframe to release memory\n",
    "\n",
    "\n",
    "    # Convert BatchLabNumber to int\n",
    "    #TeraNovaTestResult_stg['BatchLabNumber'] = TeraNovaTestResult_stg['BatchLabNumber'].astype('int')\n",
    "\n",
    "    # Convert SpecimentAgeTest to integer\n",
    "    TeraNovaTestResult_stg['SpecimenAgeTested'] = TeraNovaTestResult_stg['SpecimenAgeTested'].astype('int')\n",
    "\n",
    "\n",
    "    # Remove the outbound rows\n",
    "    TeraNovaTestResult_stg = TeraNovaTestResult_stg[(TeraNovaTestResult_stg['SpecimenAgeTested'] >= 1) & (TeraNovaTestResult_stg['SpecimenAgeTested'] <= age_day_max)]\n",
    "\n",
    "\n",
    "\n",
    "    #################################### Save only the Latest Data ############################################\n",
    "\n",
    "    TeraNovaFieldReport_stg  = TeraNovaFieldReport_stg[TeraNovaFieldReport_stg.groupby('BatchLabNumber')['ReportDateIssued'].transform('max') == TeraNovaFieldReport_stg['ReportDateIssued']].reset_index(drop=True)\n",
    "\n",
    "    TeraNovaTestResult_stg  = TeraNovaTestResult_stg[TeraNovaTestResult_stg.groupby('BatchLabNumber')['ReportDateIssued'].transform('max') == TeraNovaTestResult_stg['ReportDateIssued']].reset_index(drop=True)\n",
    "\n",
    "    # Drop NA values from the compression test report\n",
    "    TeraNovaTestResult_stg = TeraNovaTestResult_stg.dropna()\n",
    "\n",
    "    # Convert SpecimenMeasuredStrength to integer\n",
    "    TeraNovaTestResult_stg['SpecimenMeasuredStrength'] = TeraNovaTestResult_stg['SpecimenMeasuredStrength'].astype('int')\n",
    "\n",
    "    # Add a new column called CylinderTestID\n",
    "    TeraNovaTestResult_stg['CylinderTestID'] = TeraNovaTestResult_stg.sort_values(['ReportDateIssued','SpecimenAgeTested'], ascending=[True,True]) \\\n",
    "                 .groupby(['BatchLabNumber']) \\\n",
    "                 .cumcount() + 1\n",
    "    \n",
    "    # Reorder field names\n",
    "    TeraNovaFieldReport_stg = TeraNovaFieldReport_stg[[\"ProjectID\"\n",
    "                                                     , \"ReportScraperID\" \n",
    "                                                     ,\"ReportFileName\"\n",
    "                                                     , \"ReportDateIssued\"\n",
    "                                                     , \"ReportTimeIngested\"\n",
    "                                                     , \"SiteTemperature\"\n",
    "                                                     , \"SiteWeather\"\n",
    "                                                     , \"ConcreteSupplier\"\n",
    "                                                     , \"ConcretePlacementLocation\"\n",
    "                                                     , \"BatchLabNumber\"\n",
    "                                                     , \"BatchMixID\"\n",
    "                                                     , \"BatchTimeMolded\"\n",
    "                                                     , \"BatchSpecimenSize\"\n",
    "                                                     , \"BatchTemperature\"\n",
    "                                                     , \"BatchUnitWeight\"\n",
    "                                                     , \"BatchRequiredStrength\"\n",
    "                                                     , \"BatchAirContent\"\n",
    "                                                     , \"BatchtSlump\"\n",
    "                                                     , \"BatchWaterAdded\"\n",
    "                                                     , \"Date_Molded\"\n",
    "                                                     , \"Time_Molded\"\n",
    "                                                     , \"BatchLabID\"]]\n",
    "    \n",
    "\n",
    "    TeraNovaFieldReport_stg['LightWeight'] = 0\n",
    "\n",
    "    \n",
    "    return TeraNovaFieldReport_stg,TeraNovaTestResult_stg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54079d3",
   "metadata": {},
   "source": [
    "#### Branson Exoport text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9c8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_field_report_table(path, text, repeated,Bran_log_file,prjct_id,scrpr_id):\n",
    "\n",
    "  \n",
    "    Project_Name = Mix_id = City = Date_casted = Date_Report_Issued = Weather = Temperature_Ambient = concrete_supplier = ''\n",
    "    Req_load = Location = Structural_Element = Mix_id = Building_Height = Water_Added = Admixtures_noted = specimen_size = ''\n",
    "    Report_time_ingested = ''\n",
    "\n",
    "\n",
    "    # Fetching the file name\n",
    "    File_name = Path(path).stem\n",
    "\n",
    "    # Data Ingestion_datetime\n",
    "    Report_time_ingested = pd.to_datetime('today').strftime(\"%Y-%m-%d %I:%M:%S %p\")\n",
    "\n",
    "\n",
    "    for row in text.split('\\n'):\n",
    "        \n",
    "        #Date & Cylinder Pick-up Date:\n",
    "        if Date_casted == '':\n",
    "            x_d = re.search(\"Date:\", row)\n",
    "            if x_d:\n",
    "                x_pick = re.search(\"Pick-up Date:\", row)\n",
    "                if x_pick:\n",
    "                    Cylinder_pick_up_date = row.split(\"Pick-up Date:\",1)[1].strip()\n",
    "                else:\n",
    "                    Date_casted = row.split(\"Date:\",1)[1].strip()\n",
    "            else:\n",
    "                Date_casted == ''\n",
    "\n",
    "\n",
    "        #Date Report Issued & Date tested\n",
    "        x_date = re.search(\"Date\", row)\n",
    "        x_D = re.search(\"DATE\", row)\n",
    "        if x_date:\n",
    "            x_rep = re.search(\"Date Report Issued:\", row)\n",
    "            if x_rep:\n",
    "                Report_Issued = row.split(\"Date Report Issued:\",1)[1].strip()\n",
    "\n",
    "                # Changing the date format:\n",
    "                dt_issued = datetime.strptime(Report_Issued, '%B %d, %Y').date()\n",
    "                Date_Report_Issued = dt_issued.strftime('%Y-%m-%d %H:%M:%S')\n",
    "  \n",
    "        elif x_D:\n",
    "            Date_Tested = row.split(\"DATE TESTED\",1)[1].strip()\n",
    "\n",
    "\n",
    "        #Weather\n",
    "        if Weather == '':\n",
    "            x_w = re.search(\"Weather\", row)\n",
    "            if x_w:\n",
    "                string_weather = row.replace(\" \", \"\")   \n",
    "                start = 'Weather:'\n",
    "                end = 'Air'\n",
    "                Weather = (string_weather.split(start))[1].split(end)[0].strip()\n",
    "            else:\n",
    "                Weather = ''\n",
    "\n",
    "  \n",
    "        #Temperature_Ambient\n",
    "        if Temperature_Ambient == '':\n",
    "            x = re.search(\"Air\", row)\n",
    "            if x:\n",
    "                string_air = row.replace(\" \", \"\")  \n",
    "\n",
    "                AirTemp = re.search(\"AirTemperature:\", string_air)\n",
    "                if AirTemp:\n",
    "                    Temperature_Ambient = string_air[AirTemp.end():][:-3]    \n",
    "                else:\n",
    "                    Temperature_Ambient = ''\n",
    "\n",
    "        #Location of Placement\n",
    "        if Location == '':\n",
    "            x_loc = re.search(\"Locations:\", row)\n",
    "            if x_loc:\n",
    "                Location = row.split(\"Locations:\",1)[1].strip()                  \n",
    "            else:\n",
    "                Location = ''\n",
    "\n",
    "    \n",
    "        #Mix ID\n",
    "        if Mix_id == '':\n",
    "            x_mix = re.search(\"Mix\", row)\n",
    "            if x_mix:\n",
    "                string_mix = row.replace(\" \", \"\")\n",
    "                start = 'Mix'\n",
    "                end = '.'\n",
    "                mixid = (string_mix.split(start))[1].split(end)[0]\n",
    "                Mix_id = re.sub('\\D', '', mixid)\n",
    "            else:\n",
    "                Mix_id = ''\n",
    "\n",
    "\n",
    "        #Water Added\n",
    "        if Water_Added == '':\n",
    "            x_water = re.search(\"Water\", row, re.IGNORECASE)\n",
    "            if x_water:\n",
    "                Water_Added = \"Water found\"\n",
    "            else:\n",
    "                Water_Added = ''\n",
    "\n",
    "    \n",
    "        #Any Admixtures noted, \n",
    "        if Admixtures_noted == '': \n",
    "            x_ad_mix = re.search(\"Admix\", row, re.IGNORECASE)\n",
    "\n",
    "            if x_ad_mix:\n",
    "                Admixtures_noted = \"Admixtures noted is found\"\n",
    "            else:\n",
    "                Admixtures_noted = ''\n",
    "\n",
    "\n",
    "        #Required Load\n",
    "        if Req_load == '':\n",
    "            x = re.search(\"CLASS\", row)\n",
    "            if x:\n",
    "                string_req_load = row.replace(\" \", \"\")  \n",
    "                start = 'CLASS(PSI)'\n",
    "                end = 'Max/Min'\n",
    "                Req_load = (string_req_load.split(start))[1].split(end)[0].strip()\n",
    "            else:\n",
    "                Req_load = ''\n",
    "\n",
    "\n",
    "        #Concrete supplier\n",
    "        if concrete_supplier == '':\n",
    "            x = re.search(\"Supplier\", row)\n",
    "            if x:\n",
    "                string_supplier = row.replace(\" \", \"\")  \n",
    "                start = 'ConcreteSupplier:'\n",
    "                end = 'CylinderPick'\n",
    "                concrete_supplier = (string_supplier.split(start))[1].split(end)[0].strip()\n",
    "            else:\n",
    "                concrete_supplier = ''\n",
    "    \n",
    " \n",
    "        #Specimen size\n",
    "        if specimen_size == '':\n",
    "            x = re.search(\"Field Set\", row)\n",
    "  \n",
    "            if x:\n",
    "                string_size = row.replace(\" \", \"\")  \n",
    "                start = 'FieldSet('\n",
    "                end = ')'\n",
    "                size = (string_size.split(start))[1].split(end)[0].strip().replace('\"', \"\")\n",
    "                specimen_size = re.sub(\"[^A-Z0-9]\", \"\", size,0,re.IGNORECASE)\n",
    "            else:\n",
    "                specimen_size = ''\n",
    "                \n",
    "        # PROJECT ID\n",
    "        ProjectID = prjct_id\n",
    "        \n",
    "        # SCRAPER ID\n",
    "        ReportScraperID = scrpr_id\n",
    "\n",
    "\n",
    "\n",
    "  #### IMPORTING FIELDS TO A DATA FRAME ####\n",
    "    Table_1_header = [\"ProjectID\", \"ReportScraperID\" ,\"ReportFileName\", \"ReportTimeIngested\", \"Date_Molded\", \"ReportDateIssued\", \"SiteWeather\", \"SiteTemperature\", \"ConcreteSupplier\", \"BatchRequiredStrength\", \n",
    "                  \"ConcretePlacementLocation\", \"BatchMixID\",  \"BatchWaterAdded\", \"BatchAdmixturesAdded\", \"BatchSpecimenSize\" ]\n",
    "\n",
    "    Values = [prjct_id,scrpr_id,File_name, Report_time_ingested, Date_casted, Date_Report_Issued, Weather, Temperature_Ambient, concrete_supplier, Req_load, \n",
    "         Location, Mix_id, Water_Added, Admixtures_noted, specimen_size]\n",
    "\n",
    "    new_row_table_1 = pd.DataFrame(Values, Table_1_header).T\n",
    "\n",
    "  \n",
    "\n",
    "  ##############  TABLE 2  ################\n",
    "\n",
    "\n",
    "    all_tables = cam.read_pdf(path, pages = 'all', flavor = 'lattice', copy_text=['h'])\n",
    "\n",
    "    x_table_field = []\n",
    "\n",
    "    for i in range(all_tables.n):\n",
    "        \n",
    "        if 0 < i <= repeated:\n",
    "            \n",
    "            tab = all_tables[i].df\n",
    "\n",
    "            #Drop unwanted columns:\n",
    "            tab_new = tab.drop(labels=[1,2,3], axis=0)  \n",
    "\n",
    "            #Converting the dataframe to string\n",
    "            tab_new = tab_new.astype(str)\n",
    "\n",
    "            # Replacing empty spaces with Null Values\n",
    "            tab_new = tab_new.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "            # Dropping the columns with all Null values:\n",
    "            tab_new = tab_new.dropna(axis=1, how='all')\n",
    "\n",
    "            # Transpose\n",
    "            tab_tran = tab_new.T\n",
    "\n",
    "            # Converting first row into header:\n",
    "            headers = tab_tran.iloc[0]\n",
    "            tab_field = pd.DataFrame(tab_tran.values[1:], columns = headers) \n",
    "\n",
    "            tab_field['Field Set (4”x8”) #'] = tab_field['Field Set (4”x8”) #'].str.replace(r'[^\\d.]+', '')\n",
    "\n",
    "            # Replacing the names of first column elements to 'BatchLabNumber' and 'Density' fields:\n",
    "            df_Table_2 =  tab_field\n",
    "            df_Table_2 = df_Table_2.rename(columns={'Field Set (4”x8”) #': 'BatchLabNumber'})\n",
    "            df_Table_2 = df_Table_2.rename(columns={'Density (lbs/cu ft)': 'Unit_Weight (lbs/cu ft)'}) \n",
    "\n",
    "\n",
    "\n",
    "            # Chaging the format of 'Time Discharge' and making the values NULL, for invalid time formats\n",
    "            # And storing them in logfiles.\n",
    "            df_Table_2['Time Discharge'] = df_Table_2['Time Discharge'].str.replace('\\D', '', regex=True)     # Removind values other than digits\n",
    "      \n",
    "            for index in range(len(df_Table_2)):\n",
    "            \n",
    "                time_dis = df_Table_2.loc[index, 'Time Discharge']\n",
    "        \n",
    "                if len(time_dis) == 4:\n",
    "                    time_conv = datetime.strptime(time_dis, '%H%M').time()\n",
    "                    t = time_conv.strftime('%H:%M:%S')\n",
    "                    t1 = time_conv.strftime('%I:%M %p')\n",
    "                    df_Table_2.loc[index,'Time Discharge'] = t1\n",
    "                    df_Table_2.loc[index, 'BatchTimeMolded'] = t\n",
    "\n",
    "                else:\n",
    "                    lab_number = df_Table_2.loc[index,'BatchLabNumber']\n",
    "                    #df_Table_2.drop(df_Table_2.index[i], inplace = True)\n",
    "                    df_Table_2.loc[index,'Time Discharge'] = ''                                         #replace with np.nan\n",
    "                    df_Table_2.loc[index, 'BatchTimeMolded'] = ''                                     #replace with np.nan\n",
    "          \n",
    "                    Comment = 'Passed NULL value for Time molded for the lab no', lab_number, ' due to incorrect time format'\n",
    "                    Bran_log_file = Bran_log_file.append({'File Name' : File_name, 'Comment' : Comment}, ignore_index = True)\n",
    "\n",
    "\n",
    "            # Replacing the name of the columns\n",
    "            df_Table_2.rename(columns={\"Air Content (%)\": \"BatchAirContent\"}, inplace=True)\n",
    "            df_Table_2.rename(columns={\"Slump (inches)\": \"BatchtSlump\"}, inplace=True)\n",
    "            df_Table_2.rename(columns={\"Unit_Weight (lbs/cu ft)\": \"BatchUnitWeight\"}, inplace=True)\n",
    "            df_Table_2.rename(columns={\"Concrete Temperature (°F)\": \"BatchTemperature\"}, inplace=True)\n",
    "            df_Table_2.rename(columns={\"Time Discharge\": \"Time_Molded\"}, inplace=True)\n",
    "\n",
    "\n",
    "            if i == 1:\n",
    "                x_table_field = df_Table_2\n",
    "            else:\n",
    "                x_table_field = pd.concat([x_table_field, df_Table_2], axis=0, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    # MERGING BOTH THE TABLES\n",
    "  \n",
    "    tables_merged = pd.concat([new_row_table_1, x_table_field], axis=1)\n",
    "\n",
    "    tables_merged[\"ReportFileName\"] = File_name \n",
    "    tables_merged[\"ReportTimeIngested\"] = Report_time_ingested\n",
    "    tables_merged[\"Date_Molded\"] = Date_casted\n",
    "    tables_merged[\"ReportDateIssued\"] = Date_Report_Issued\n",
    "    tables_merged[\"SiteWeather\"] = Weather\n",
    "    tables_merged[\"SiteTemperature\"] = Temperature_Ambient\n",
    "    tables_merged[\"ConcreteSupplier\"] = concrete_supplier\n",
    "    tables_merged[\"BatchRequiredStrength\"] = Req_load\n",
    "    tables_merged[\"ConcretePlacementLocation\"] = Location\n",
    "    tables_merged[\"BatchMixID\"] = Mix_id\n",
    "    tables_merged[\"BatchWaterAdded\"] = Water_Added\n",
    "    tables_merged[\"Admixtures_noted\"] = Admixtures_noted\n",
    "    tables_merged[\"BatchSpecimenSize\"] = specimen_size\n",
    "    tables_merged[\"ProjectID\"] = prjct_id\n",
    "    tables_merged[\"ReportScraperID\"] = scrpr_id\n",
    "\n",
    "   \n",
    "    # Changing the date format of 'Date_Casted' field:\n",
    "    dt_cas = datetime.strptime(Date_casted, '%B %d, %Y').date()\n",
    "    new_date_format = dt_cas.strftime('%Y-%m-%d')\n",
    "\n",
    " \n",
    "    # Batch_time_molded field:\n",
    "\n",
    "    for count in range(len(tables_merged)):\n",
    "        if tables_merged.loc[count, 'BatchTimeMolded'] != '':\n",
    "            tables_merged.loc[count, 'BatchTimeMolded'] = new_date_format + ' ' + tables_merged.loc[count, 'BatchTimeMolded']\n",
    "    \n",
    "    field_report_table = tables_merged\n",
    "\n",
    "\n",
    "    # Reordering the dataframe columns\n",
    "    field_report_table = field_report_table[[\"ProjectID\", \"ReportScraperID\" ,'ReportFileName', \"ReportDateIssued\", \"ReportTimeIngested\", \"SiteTemperature\", \"SiteWeather\", \n",
    "                      \"ConcreteSupplier\", \"ConcretePlacementLocation\", \"BatchLabNumber\", \"BatchMixID\", \"BatchTimeMolded\",  \n",
    "                      \"BatchSpecimenSize\", \"BatchTemperature\", \"BatchUnitWeight\", \"BatchRequiredStrength\",  \"BatchAirContent\",\n",
    "                      \"BatchtSlump\", \"BatchWaterAdded\", \"BatchAdmixturesAdded\", \"Date_Molded\", \"Time_Molded\"]]\n",
    "\n",
    "\n",
    "\n",
    "    return field_report_table, Bran_log_file, Date_Report_Issued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28131a69",
   "metadata": {},
   "source": [
    "#### Branson Extract Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7896bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_compression_table(path, text, repeated, Date_Report_Issued):\n",
    "    read_all_tables = cam.read_pdf(path, pages = 'all', flavor = 'lattice', copy_text=['h'])\n",
    "\n",
    "    x_table = []\n",
    "\n",
    "    for i in range(read_all_tables.n):\n",
    "        if i <= repeated:\n",
    "            continue\n",
    "        else:\n",
    "            tab = read_all_tables[i].df\n",
    "\n",
    "            #Converting the dataframe to string\n",
    "            tab = tab.astype(str)\n",
    "\n",
    "            # Replacing empty spaces with Null Values\n",
    "            tab = tab.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "            # Dropping the columns with all Null values:\n",
    "            tab = tab.dropna(axis=1, how='all')\n",
    "\n",
    "            #Drop unwanted columns:\n",
    "            tab_new = tab.drop(labels=[2,3,6,8,9,10,11], axis=0)\n",
    "            tab_new_trans = tab_new.T\n",
    "\n",
    "            # Converting first row into header:\n",
    "            # Changing name of the first header element to 'BatchLabNumber'\n",
    "            headers = tab_new_trans.iloc[0]\n",
    "            tab_final = pd.DataFrame(tab_new_trans.values[1:], columns = headers)\n",
    "            tab_final.columns.values[0:1] =['BatchLabNumber']\n",
    "\n",
    "            # Changing the name of the field\n",
    "            tab_final = tab_final.rename(columns={'PSI': 'Unit_Load (PSI)'})\n",
    "            tab_final = tab_final.rename(columns={'Unit_Load (PSI)': 'Unit_load (lbs/sq in)'})\n",
    "            tab_final['BatchLabNumber'] = tab_final['BatchLabNumber'].str.replace(r'[^\\d.]+', '')\n",
    "\n",
    "            # Dropping the rows with if any Null values:\n",
    "            tab_final = tab_final.dropna()\n",
    "      \n",
    "            if i == repeated + 1:\n",
    "                x_table = tab_final                                                  \n",
    "            else:\n",
    "                x_table = pd.concat([x_table, tab_final], axis=0, ignore_index=True)\n",
    "\n",
    "  \n",
    "    x_table['ReportDateIssued'] = Date_Report_Issued\n",
    "\n",
    "    '''\n",
    "      # Add a new column 'CylinderTestID':\n",
    "      x_table['CylinderTestID'] = x_table.sort_values(['ReportDateIssued','TEST AT (days)'], ascending=[True,True]) \\\n",
    "                 .groupby(['BatchLabNumber']).cumcount() + 1\n",
    "\n",
    "\n",
    "      # Replacing the name of the columns\n",
    "      x_table.rename(columns={\"CYLINDER #\": \"Cylinder_Number\"}, inplace=True)\n",
    "    '''\n",
    "\n",
    "    x_table.rename(columns={\"CYLINDER #\": \"Cylinder_Number\"}, inplace=True)\n",
    "    x_table.rename(columns={\"DATE TESTED\": \"SpecimenDateTested\"}, inplace=True)\n",
    "    x_table.rename(columns={\"TEST AT (days)\": \"SpecimenAgeTested\"}, inplace=True)\n",
    "    x_table.rename(columns={\"Unit_load (lbs/sq in)\": \"SpecimenMeasuredStrength\"}, inplace=True)\n",
    "  \n",
    "\n",
    "    compression_table = x_table\n",
    "\n",
    "    return compression_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f83bc3",
   "metadata": {},
   "source": [
    "#### Branson Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636ab10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Branson(folder_dir,prjct_id,scrpr_id):\\\n",
    "    \n",
    "    field_report_table = []\n",
    "    compression_table = []\n",
    "    time_format = ''\n",
    "    res = ''\n",
    "\n",
    "    # Creating a log dataframes:\n",
    "    Bran_invalid_file_log = pd.DataFrame(columns=['File Name', 'Comment'])\n",
    "    Bran_log_file = pd.DataFrame(columns=['File Name', 'Comment'])                       # Final log file\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_dir,topdown=True):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                # Skipping the files with age days other than 7 or 28 days,  and maintaining them in Log table\n",
    "                res = re.findall(r'\\(.*?\\)', file)\n",
    "                if not '7' in str(res):\n",
    "                    if not '28' in str(res):\n",
    "                        Bran_invalid_file_log = Bran_invalid_file_log.append({'File Name' : file, 'Comment' : 'Invalid file with Age days other than 7 or 28'}, ignore_index = True)\n",
    "                        continue\n",
    "\n",
    "                bran_path = root.replace('\\\\','/')+'/'+file\n",
    "                pdf = pdfplumber.open(bran_path)\n",
    "                page = pdf.pages[0]\n",
    "                text = page.extract_text()\n",
    "\n",
    "                # Cheking the number of times the string 'Field Set' is repeated, \n",
    "                # to count the number of 'Concrete Field Inspection and Test Results' tables:\n",
    "        \n",
    "                repeated = text.count(\"Field Set\")\n",
    "\n",
    "                # Importing data to the function\n",
    "                extracted_first, Bran_log_file, Date_Report_Issued = extract_field_report_table(bran_path, text, repeated,Bran_log_file,prjct_id,scrpr_id)\n",
    "                extracted_second = extract_compression_table(bran_path, text, repeated, Date_Report_Issued)\n",
    "\n",
    "                # Appending data into tables:\n",
    "                field_report_table = extracted_first.append(field_report_table, ignore_index=True)\n",
    "                compression_table = extracted_second.append(compression_table, ignore_index=True)\n",
    "        \n",
    "                # Appendind data into log_file\n",
    "                Bran_log_file = pd.concat([Bran_log_file, Bran_invalid_file_log], axis=0, ignore_index=True)\n",
    "\n",
    "        #return name, field_report_table, compression_table, Bran_log_file\n",
    "    return field_report_table, compression_table, Bran_log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f4834",
   "metadata": {},
   "source": [
    "#### Branson Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4318311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Branson_data_transform(table_1, table_2, table_log, Directory):\n",
    "\n",
    "\n",
    "    os.chdir(ExportDir)\n",
    "\n",
    "    # Delteing duplicate entries from field_report_table (SpecimenFieldReport):\n",
    "    Branson_FieldReportTable  = table_1[table_1.groupby('BatchLabNumber')['ReportDateIssued'].transform('max') == table_1['ReportDateIssued']].reset_index(drop=True)\n",
    "\n",
    "    # Delteing duplicate entries from compression_table (SpecimenCompressionTestResult):\n",
    "    Branson_CompressionResults  = table_2[table_2.groupby('BatchLabNumber')['ReportDateIssued'].transform('max') == table_2['ReportDateIssued']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Add a new column CylinderTestID\n",
    "    Branson_CompressionResults['CylinderTestID'] = Branson_CompressionResults.sort_values(['ReportDateIssued','SpecimenAgeTested'], ascending=[True,True]) \\\n",
    "    .groupby(['BatchLabNumber']).cumcount() + 1\n",
    "\n",
    "\n",
    "    # generate the uuid for BatchLabID \n",
    "    \n",
    "    batchlabID = {BatchLabNumber: str(uuid.uuid4()) for BatchLabNumber in Branson_FieldReportTable['BatchLabNumber'].unique()}\n",
    "    Branson_FieldReportTable['BatchLabID'] = Branson_FieldReportTable['BatchLabNumber'].map(batchlabID)\n",
    "    Branson_CompressionResults['BatchLabID'] = Branson_CompressionResults['BatchLabNumber'].map(batchlabID)\n",
    "\n",
    "  \n",
    "    # Changing the datatypes of all the fields from Field report table:\n",
    "    Branson_FieldReportTable['ReportTimeIngested'] = Branson_FieldReportTable['ReportTimeIngested'].astype('datetime64[ns]')\n",
    "    Branson_FieldReportTable['ReportDateIssued'] = pd.to_datetime(Branson_FieldReportTable[\"ReportDateIssued\"]).dt.floor('D')\n",
    "    Branson_FieldReportTable['BatchTimeMolded'] = Branson_FieldReportTable['BatchTimeMolded'].astype('datetime64[ns]')\n",
    "\n",
    "    Branson_FieldReportTable['SiteTemperature'] = Branson_FieldReportTable['SiteTemperature'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    Branson_FieldReportTable['SiteTemperature'] = Branson_FieldReportTable['SiteTemperature'].astype('float')\n",
    "    Branson_FieldReportTable['BatchTemperature'] = Branson_FieldReportTable['BatchTemperature'].astype('float')\n",
    "    Branson_FieldReportTable['BatchUnitWeight'] = Branson_FieldReportTable['BatchUnitWeight'].astype('float')\n",
    "    #Branson_FieldReportTable['BatchRequiredStrength'] = Branson_FieldReportTable['BatchRequiredStrength'].astype('float')\n",
    "    Branson_FieldReportTable['BatchAirContent'] = Branson_FieldReportTable['BatchAirContent'].astype('float')\n",
    "    Branson_FieldReportTable['BatchtSlump'] = Branson_FieldReportTable['BatchtSlump'].astype('float')\n",
    "    #Branson_FieldReportTable['BatchWaterAdded'] = Branson_FieldReportTable['BatchWaterAdded'].astype('float')\n",
    "\n",
    "\n",
    "    # Changing the datatypes of all the fields from Compression test results table:\n",
    "    Branson_CompressionResults['SpecimenAgeTested'] = Branson_CompressionResults['SpecimenAgeTested'].astype('int')\n",
    "    Branson_CompressionResults['ReportDateIssued'] = pd.to_datetime(Branson_CompressionResults[\"ReportDateIssued\"]).dt.floor('D')\n",
    "    Branson_CompressionResults['SpecimenMeasuredStrength'] = Branson_CompressionResults['SpecimenMeasuredStrength'].astype('int')\n",
    "    Branson_CompressionResults['SpecimenDateTested'] = pd.to_datetime(Branson_CompressionResults[\"SpecimenDateTested\"], format=\"%m/%d/%y\").dt.floor('D')\n",
    "    \n",
    "    # Add another column for Light Weight\n",
    "    LightWeight = []\n",
    "\n",
    "    for idx, row in Branson_FieldReportTable.iterrows():\n",
    "        x_lw = re.search('LW', row['BatchRequiredStrength'])\n",
    "        \n",
    "        if x_lw:\n",
    "            value = row['BatchRequiredStrength'].replace('LW', '')\n",
    "            Branson_FieldReportTable['BatchRequiredStrength'][idx] = value\n",
    "            LightWeight.append(1)\n",
    "        else:\n",
    "            LightWeight.append(0)\n",
    " \n",
    "    Branson_FieldReportTable['LightWeight'] = LightWeight\n",
    "\n",
    "    # Change datatype to float\n",
    "    Branson_FieldReportTable['BatchRequiredStrength'] = Branson_FieldReportTable['BatchRequiredStrength'].astype('float')\n",
    "\n",
    "\n",
    "    return Branson_FieldReportTable, Branson_CompressionResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe02eb0",
   "metadata": {},
   "source": [
    "#### Export Data Files Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb5450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportDataFiles(TeraNovaFieldReport_Final,TeraNovaTestResult_Final,TeraNovaFileLog,ExportDir,engineer_report,Branson_FieldReportTable,Branson_CompressionResults,final_log_file):\n",
    "    \n",
    "    os.chdir(ExportDir)   # Set the working directory\n",
    "    \n",
    "    # Exoport Updated Project Information File\n",
    "    engineer_report.to_excel('ProjectInformationUpdated.xlsx', index=False)\n",
    "    \n",
    "    # Exporting SpecimenCompressionTestResult to excel:\n",
    "    if len(Branson_CompressionResults.index) == 0 and len(TeraNovaTestResult_Final.index) == 0:\n",
    "        pass\n",
    "        \n",
    "    elif len(Branson_CompressionResults.index) == 0 and len(TeraNovaTestResult_Final.index) != 0:\n",
    "        TeraNovaTestResult_Final.to_excel('SpecimenCompressionTestResult.xlsx', index=False)\n",
    "        \n",
    "    elif len(Branson_CompressionResults.index) != 0 and len(TeraNovaTestResult_Final.index) == 0:\n",
    "        Branson_CompressionResults.to_excel('SpecimenCompressionTestResult.xlsx', index=False)\n",
    "        \n",
    "    else:\n",
    "        pd.concat([TeraNovaTestResult_Final\n",
    "                  ,Branson_CompressionResults.drop(['Cylinder_Number'], axis=1)]\n",
    "              , ignore_index=True).to_excel('SpecimenCompressionTestResult.xlsx', index=False)\n",
    "    \n",
    "    \n",
    "     # Exporting SpecimenFieldReport data to excel:\n",
    "    if len(Branson_FieldReportTable.index) == 0 and len(TeraNovaFieldReport_Final.index) == 0:\n",
    "        pass\n",
    "        \n",
    "    elif len(Branson_FieldReportTable.index) == 0 and len(TeraNovaFieldReport_Final.index) != 0:\n",
    "        TeraNovaFieldReport_Final.drop([\"Date_Molded\", \"Time_Molded\" ]\n",
    "                                       , axis=1).to_excel('SpecimenFieldReport.xlsx', index=False)\n",
    "        \n",
    "    elif len(Branson_FieldReportTable.index) != 0 and len(TeraNovaFieldReport_Final.index) == 0:\n",
    "        Branson_FieldReportTable.drop([\"BatchAdmixturesAdded\",\"Date_Molded\", \"Time_Molded\"]\n",
    "                                      , axis=1).to_excel('SpecimenFieldReport.xlsx', index=False)\n",
    "    else:\n",
    "        pd.concat([TeraNovaFieldReport_Final.drop([\"Date_Molded\", \"Time_Molded\" ], axis=1)\n",
    "                  ,Branson_FieldReportTable.drop([\"BatchAdmixturesAdded\",\"Date_Molded\", \"Time_Molded\"], axis=1)]\n",
    "              , ignore_index=True).to_excel('SpecimenFieldReport.xlsx', index=False) \n",
    "    \n",
    "    if len(final_log_file.index) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        final_log_file.to_excel(r'Branson_bad_data_log_report.xlsx', index=False)\n",
    "               \n",
    "#         TeraNovaFileLog.to_excel('INGESTION_LOG.xlsx', index=False)\n",
    "#         final_log_file.to_excel(r'log_report.xlsx', index=False)\n",
    "\n",
    "    return 'Excel Files Exported Successfully'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f2979",
   "metadata": {},
   "source": [
    "#### Export Job Log File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e088f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JobLog(TeraNovaFileLog,TeraNovaFieldReport,TeraNovaTestResult,TeraNovaFieldReport_Final,TeraNovaTestResult_Final,jobTimeDiff,ExportDir):\n",
    "    \n",
    "    os.chdir(ExportDir)\n",
    "    \n",
    "    if len(TeraNovaFieldReport_Final.index) and len(Branson_FieldReportTable.index) != 0:\n",
    "        with open('JobComplete.log', 'w') as f:\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t',date.today().strftime(\"%A,%d %B, %Y\"), file=f)\n",
    "            print('\\t\\t\\t\\t       ',datetime.now().time().strftime(\"%I:%M:%S %p\"), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t  TeraNova Scrapper Logs', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('Total Files Read:', len(TeraNovaFileLog), file=f)\n",
    "            print('Total Reports Read:', len(TeraNovaFieldReport), file=f)\n",
    "            print('Total Test Obervations Read:', len(TeraNovaTestResult), file=f)\n",
    "            print('----------------- Ingestion of Reports with Latest Reporting Date ----------------------', file=f)\n",
    "            print('Total Reports Ingested:', len(TeraNovaFieldReport_Final), file=f)\n",
    "            print('----------------- Ingestion of Acceptable and Latest Test Results ----------------------', file=f)\n",
    "            print('Total Test Results Ingested:', len(TeraNovaTestResult_Final), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t  Branson Scrapper Logs', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('----------------- Ingestion of Reports with Latest Reporting Date ----------------------', file=f)\n",
    "            print('Total Reports Read and Ingested:', len(Branson_FieldReportTable),file=f)\n",
    "            print('----------------- Ingestion of Acceptable and Latest Test Results ----------------------', file=f)\n",
    "            print('Total Test Results Read and Ingested:', len(Branson_CompressionResults) ,file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print(\"\\t\\t\\tTotal job took %d hours %d minutes %d seconds\" % (jobTimeDiff.hours, jobTimeDiff.minutes, jobTimeDiff.seconds), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "        \n",
    "    elif len(TeraNovaFieldReport_Final.index) == 0 and len(Branson_FieldReportTable.index) != 0:\n",
    "        with open('JobComplete.log', 'w') as f:\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t',date.today().strftime(\"%A,%d %B, %Y\"), file=f)\n",
    "            print('\\t\\t\\t\\t       ',datetime.now().time().strftime(\"%I:%M:%S %p\"), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t No TeraNova Files Read', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t  Branson Scrapper Logs', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('----------------- Ingestion of Reports with Latest Reporting Date ----------------------', file=f)\n",
    "            print('Total Reports Read and Ingested:', len(Branson_FieldReportTable),file=f)\n",
    "            print('----------------- Ingestion of Acceptable and Latest Test Results ----------------------', file=f)\n",
    "            print('Total Test Results Read and Ingested:', len(Branson_CompressionResults) ,file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print(\"\\t\\t\\tTotal job took %d hours %d minutes %d seconds\" % (jobTimeDiff.hours, jobTimeDiff.minutes, jobTimeDiff.seconds), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "    elif len(TeraNovaFieldReport_Final.index) != 0 and len(Branson_FieldReportTable.index) == 0:\n",
    "        with open('JobComplete.log', 'w') as f:\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t',date.today().strftime(\"%A,%d %B, %Y\"), file=f)\n",
    "            print('\\t\\t\\t\\t       ',datetime.now().time().strftime(\"%I:%M:%S %p\"), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t  TeraNova Scrapper Logs', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('Total Files Read:', len(TeraNovaFileLog), file=f)\n",
    "            print('Total Reports Read:', len(TeraNovaFieldReport), file=f)\n",
    "            print('Total Test Obervations Read:', len(TeraNovaTestResult), file=f)\n",
    "            print('----------------- Ingestion of Reports with Latest Reporting Date ----------------------', file=f)\n",
    "            print('Total Reports Ingested:', len(TeraNovaFieldReport_Final), file=f)\n",
    "            print('----------------- Ingestion of Acceptable and Latest Test Results ----------------------', file=f)\n",
    "            print('Total Test Results Ingested:', len(TeraNovaTestResult_Final), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t No Branson Files Read', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print(\"\\t\\t\\tTotal job took %d hours %d minutes %d seconds\" % (jobTimeDiff.hours, jobTimeDiff.minutes, jobTimeDiff.seconds), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "    else:\n",
    "         with open('JobComplete.log', 'w') as f:\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t',date.today().strftime(\"%A,%d %B, %Y\"), file=f)\n",
    "            print('\\t\\t\\t\\t       ',datetime.now().time().strftime(\"%I:%M:%S %p\"), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print('\\t\\t\\t\\t No TeraNova or Branson file Read', file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "            print(\"\\t\\t\\tTotal job took %d hours %d minutes %d seconds\" % (jobTimeDiff.hours, jobTimeDiff.minutes, jobTimeDiff.seconds), file=f)\n",
    "            print('========================================================================================', file=f)\n",
    "        \n",
    "            \n",
    "        \n",
    "    return 'Check JobComplete.log File'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c8361",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe9ea968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check JobComplete.log File'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note Down the Job Start DateTime\n",
    "jobStart = datetime.now()\n",
    "\n",
    "# Save Export Files Directory\n",
    "ExportDir = os.getcwd()\n",
    "\n",
    "# Read the Engineer's File\n",
    "engineer_report = pd.read_excel('ProjectInformation.xlsx')\n",
    "\n",
    "# generate the uuid for ProjectID\n",
    "prjctid = {DCEProjectNumber: str(uuid.uuid4()) for DCEProjectNumber in engineer_report['DCEProjectNumber'].unique()}\n",
    "\n",
    "# map uuid\n",
    "engineer_report['ProjectID'] = engineer_report['DCEProjectNumber'].map(prjctid)\n",
    "\n",
    "# Declare TeraNovaDatframes\n",
    "TeraNovaFieldReport = pd.DataFrame()\n",
    "TeraNovaTestResult = pd.DataFrame()\n",
    "TeraNovaFileLog = pd.DataFrame()\n",
    "TeraNovaFieldReport_Final = pd.DataFrame()\n",
    "TeraNovaTestResult_Final = pd.DataFrame()\n",
    "\n",
    "# Declare Branson Dataframes\n",
    "Branson_CompressionResults = pd.DataFrame()\n",
    "Branson_FieldReportTable = pd.DataFrame()\n",
    "final_log_file = pd.DataFrame()\n",
    "final_field_report_table = pd.DataFrame()\n",
    "final_compression_table = pd.DataFrame()\n",
    "\n",
    "# Specify a varibale for max age days to consider in the data\n",
    "age_day_max = 100\n",
    "\n",
    "for index, row in engineer_report.iterrows():\n",
    "    if (engineer_report['ScraperID'].isnull()[index]) & (engineer_report['Directory'].isnull()[index]):\n",
    "        pass\n",
    "    else:\n",
    "        scrpr_id = engineer_report['ScraperID'][index]\n",
    "        foldeR_dir = engineer_report['Directory'][index].replace('\\\\', '/')\n",
    "                \n",
    "        if scrpr_id == 1001:\n",
    "            prjct_id = engineer_report['ProjectID'][index]\n",
    "            FieldReport, TestResult, FileLog = TeraNova(foldeR_dir,prjct_id,scrpr_id)\n",
    "            \n",
    "            TeraNovaFieldReport = TeraNovaFieldReport.append(FieldReport).reset_index(drop=True) # Appends results to DF\n",
    "            TeraNovaTestResult = TeraNovaTestResult.append(TestResult).reset_index(drop=True) # Appends results to DF\n",
    "            TeraNovaFileLog = TeraNovaFileLog.append(FileLog).reset_index(drop=True) # Appends results to DF\n",
    "            \n",
    "        elif scrpr_id == 2001:\n",
    "            scrpr_id = engineer_report['ScraperID'][index]\n",
    "            foldeR_dir = engineer_report['Directory'][index].replace('\\\\', '/')\n",
    "            prjct_id = engineer_report['ProjectID'][index]\n",
    "            \n",
    "            final_field_report_table, final_compression_table, final_log_file =   Branson(foldeR_dir,prjct_id,scrpr_id)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Call TeraNova DataTransform Function\n",
    "if len(TeraNovaFieldReport.index) | len(TeraNovaTestResult.index) == 0:\n",
    "    pass\n",
    "else:\n",
    "    TeraNovaFieldReport_Final, TeraNovaTestResult_Final = TeraNovaDataTransform(TeraNovaFieldReport, TeraNovaTestResult)\n",
    "   \n",
    "    \n",
    "# Call Branson DataTransform Function\n",
    "if len(final_field_report_table.index) | len(final_compression_table.index) == 0:\n",
    "    pass\n",
    "else:\n",
    "    Branson_FieldReportTable, Branson_CompressionResults = Branson_data_transform(final_field_report_table, final_compression_table, final_log_file, ExportDir)\n",
    "\n",
    "# Call Export Data Files Function\n",
    "ExportDataFiles(TeraNovaFieldReport_Final,TeraNovaTestResult_Final,TeraNovaFileLog,ExportDir,engineer_report,Branson_FieldReportTable,Branson_CompressionResults,final_log_file)\n",
    "\n",
    "# Note Down the Job Finish DateTime\n",
    "jobStop = datetime.now()\n",
    "\n",
    "# Get the JobTime Difference\n",
    "jobTimeDiff = relativedelta(jobStop,jobStart)\n",
    "\n",
    "# Call the Jog Log File Function\n",
    "JobLog(TeraNovaFileLog,TeraNovaFieldReport,TeraNovaTestResult,TeraNovaFieldReport_Final,TeraNovaTestResult_Final,jobTimeDiff,ExportDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4058a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
